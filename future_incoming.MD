# The Future That Comes Upon Us #

The rapid development of Generative AI has introduced numerous new challenges: amplifying biases, erosion of trust, disinformation, lack of accountability, manipulation - to name a few.
When it comes to speech, the focus is often on unauthorized voice cloning. Today, I want to talk about an often-overlooked aspect: the intersection of social manipulation and generative speech.

### Prelim 0 ###

The first point I want to make is that audio differs significantly from text. I believe that audio has a more direct access to our emotional, non-rational brain.

Written text is a technology and a very recent one: the oldest known written symbols date some 10,000 years back [[1]](#reference-1). Individuals must learn to read, and this skill is acquired at a relatively later age. It takes time to cultivate the skill of visualizing concepts from text and being emotionally moved by it.

In contrast, audio communication predates our own species. Birds, reptiles, insects, and even fish communicate via sounds [[2]](#reference-2). Sound-producing animals existed hundreds of millions of years ago [[3]](#reference-3). Humans produce sounds from the first minutes of their lives,  utilizing their limited repertoire of vocalizations to communicate their needs to their caregivers. The voice of your child can produce emotional response even before one realizes _what_ is being said. Conversely, children already recognize their mother's voice even before they are born. The way babies cry already mimics the patterns of their parent's language [[4]](#reference-4).


Therefore, my hypothesis is that, compared to audio, humans possess a natural defense against being influenced by text. Just as individuals tend to be more rational when communicating in a foreign language than in their native one [[5]](#reference-5), I believe humans exhibit a greater rationality when reading text compared to hearing speech. Conversely, we can say that audio has a more direct access to our brains.


### Prelim 1 ###

Another point is that the quality of speech generation systems has rapidly become alarmingly good. Compare the capabilities from nine years ago [[6]](#reference-6) to those of three years ago [[7]](#reference-6), and then to what is available today [[8]](#reference-8), [[9]](#reference-9). These systems now offer very high audio quality, naturally-sounding prosody and emotions, faster-than-real-time generation, and include realistic non-word vocalizations. They can even perform zero-shot voice cloning from just a few seconds of audio [[10]](#reference-10), [[11]](#reference-11), [[12]](#reference-12).

It's highly probable that within just a few years, the average person will find it impossible to distinguish synthesized speech from natural human speech. In fact, it's already difficult to distinguish synthetic speech, particularly over phone calls. At the same time, the voices, prosody, paralinguistic features will be easily configurable. 

### Putting it all together ###

While the focus has largely been on challenges posed by textual Large Language Models (LLMs), I believe that it is the combination of speech generation and smart LLMs that presents an even more unsettling array of problems. Imagine an agent as smart as a SoTA reasoning LLM, optimized to manipulate or scam you and now equipped with a direct access to your emotional state. Wouldn't that make manipulation even easier?

There could be different levels of this.

Consider a scenario beginning with a highly empathetic voice answering your call to an airline support line. The voice and what is being said are optimized to reduce the number of refunds. "Yes John, I understand how that makes you feel. We are deeply sorry for the flight being delayed. Would you accept this meal voucher as an apology?" At this stage, it is perhaps not as different as optimizing the shade of the ten blue links to increase the user engagement [[13]](#reference-13).

The next level is still benign. For instance, an AI might detect your Russian accent and male voice, and adapt by deploying a Russian-accented female voice to sell you a year's worth of printing paper. Alternatively, it could switch directly to Russian, listening as you lament the difficulty of acquiring grechka in this country.

With a further optimization steps, the bot on the other end of the line could precisely time pauses and emphasize specific words or phrases, in a ways that are extremely tailored for you --- in order to subtly increase your susceptibility to a scam. Now, extrapolate this to millions of simultaneous sessions, operating 24/7 from a distant data center. Large-scale LLMs deployments weaponized to incrementally affect your purchasing decisions or straight scam you.

Or consider a cold call where the AI attempts to mimic your child's voice, based solely on your 'Hello,' to solicit money.


### Is there a way out? ###

Is there a solution to avoid this dystopia? Unfortunately, I believe the answer is no. The underlying technology is already ready; it's only a matter of time before applications and market catch-up. Likely, the market forces are already in motion, building the first stages of this.

Yet I hope that writing down my thoughts might help to start a discussion?

### References ###


<a id="reference-1">[1]</a> https://en.wikipedia.org/wiki/Jiahu_symbols

<a id="reference-2">[2]</a> https://pmc.ncbi.nlm.nih.gov/articles/PMC6519373/ --- a random paper on the topic, maybe there are better references!

<a id="reference-3">[3]</a> https://www.scientificamerican.com/article/fossils-reveal-when-animals-started-making-noise/

<a id="reference-4">[4]</a> https://www.sciencedirect.com/science/article/pii/S0960982209018247

<a id="reference-5">[5]</a> https://www.sciencedirect.com/science/article/abs/pii/S001002771300228X

<a id="reference-6">[6]</a> https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/

<a id="reference-7">[7]</a> https://google-research.github.io/seanet/soundstorm/examples/

<a id="reference-8">[8]</a> https://www.hume.ai/blog/introducing-evi-3

<a id="reference-9">[9]</a> https://elevenlabs.io/voice-library

<a id="reference-10">[10]</a> https://google-research.github.io/seanet/speartts/examples/#prompting

<a id="reference-11">[11]</a> https://elevenlabs.io/voice-cloning

<a id="reference-12">[12]</a> https://www.microsoft.com/en-us/research/project/vall-e-x/

<a id="reference-13">[13]</a> https://www.quora.com/How-much-did-Google-increase-conversions-after-its-41-shades-of-blue-test/answer/Ronny-Kohavi

